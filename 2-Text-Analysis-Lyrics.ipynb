{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "\n",
    "[Free Form Description]\n",
    "\n",
    "**Resources**\n",
    "\n",
    "- [The NRC Valence, Arousal, and Dominance Lexicon](http://saifmohammad.com/WebPages/nrc-vad.html)\n",
    "- [Blogpost on scraping song lyrics](https://chrishyland.github.io/scraping-from-genius/) - Thanks to Chris Hyland for this!\n",
    "- [Genius API documentation](https://docs.genius.com/#/getting-started-h1) \n",
    "\n",
    "\n",
    "**Data Input:**\n",
    "\n",
    "- `data/processed/audio_data.csv`: DataFrame of all CC tracks with \"Sonic Brutality Index\" (from notebook 1)\n",
    "- `data/raw/NRC-VAD-Lexicon.txt`: Data of approx 20'000 words with valence, arousal and dominance scores\n",
    "\n",
    "**Data Output:**\n",
    "\n",
    "- `...`: ...\n",
    "\n",
    "**Changes**\n",
    "\n",
    "- 2019-02-18: Start project\n",
    "- 20-02-25: Complete audio analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T21:27:45.612562Z",
     "start_time": "2020-01-20T21:27:45.472340Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import credentials # file where credentials for genius API are stored\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('raph-base')\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Lexicon\n",
    "The NRC Valence, Arousal, and Dominance (VAD) Lexicon includes a list of more than 20,000 English words and their valence, arousal, and dominance scores. For a given word and a dimension (V/A/D), the scores range from 0 (lowest V/A/D) to 1 (highest V/A/D). Reading the lexicon into a Pandas DataFrame requires a little tweaking / cleaning first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/NRC-VAD-Lexicon.txt') as file:\n",
    "    data_list = []\n",
    "    line = file.readline()\n",
    "    while line:\n",
    "        data_list.append(str(line))\n",
    "        line = file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Word\\tValence\\tArousal\\tDominance\\n',\n",
       " 'aaaaaaah\\t0.479\\t0.606\\t0.291\\n',\n",
       " 'aaaah\\t0.520\\t0.636\\t0.282\\n',\n",
       " 'aardvark\\t0.427\\t0.490\\t0.437\\n',\n",
       " 'aback\\t0.385\\t0.407\\t0.288\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results\n",
    "data_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and clean\n",
    "data_list2 = [x.replace('\\n', '').split('\\t') for x in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_lexicon = pd.DataFrame(data_list2[1:], columns=data_list2[0], dtype=float)\n",
    "vad_lexicon.columns = (col.lower() for col in vad_lexicon.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>bloodshed</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  valence  arousal  dominance\n",
       "1860  bloodshed    0.048    0.942      0.525"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check results ...\n",
    "display(vad_lexicon.iloc[[1860]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly what we are looking for: low valence, high arousal ... ;-) \n",
    "\n",
    "We can also see that `dominance` is quite neutral and probably no feature that will be of further help. To more easily filter and analize for words with a combination of low-valence and high-arousal I will create a new feature `anti-valence` that is (1 - valance). Then we can simply sum the 2 scores to get a `word brutality index (WBI)`. (To land in a range between 0 and 1 we will normalize it using sklearn's minmax_scaler.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = vad_lexicon.copy()\n",
    "lexicon['anti_valence'] = lexicon['valence'].apply(lambda x: 1-x)\n",
    "wbi = minmax_scale(lexicon['anti_valence'] + lexicon['arousal'])\n",
    "lexicon['wbi'] = wbi\n",
    "lexicon.drop(['valence', 'dominance'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>arousal</th>\n",
       "      <th>anti_valence</th>\n",
       "      <th>wbi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8472</th>\n",
       "      <td>homicide</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.990</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11521</th>\n",
       "      <td>murderer</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.992746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9854</th>\n",
       "      <td>killer</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.981585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>abduction</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.980469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17277</th>\n",
       "      <td>suicidebombing</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.979353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11523</th>\n",
       "      <td>murderous</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.977679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>dangerous</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>assassinate</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.974888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>aggresive</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.971540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>bloodbath</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.970982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  arousal  anti_valence       wbi\n",
       "8472         homicide    0.973         0.990  1.000000\n",
       "11521        murderer    0.960         0.990  0.992746\n",
       "9854           killer    0.971         0.959  0.981585\n",
       "20          abduction    0.990         0.938  0.980469\n",
       "17277  suicidebombing    0.957         0.969  0.979353\n",
       "11523       murderous    0.940         0.983  0.977679\n",
       "4366        dangerous    0.941         0.980  0.976562\n",
       "1035      assassinate    0.969         0.949  0.974888\n",
       "386         aggresive    0.971         0.941  0.971540\n",
       "1856        bloodbath    0.971         0.940  0.970982"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>arousal</th>\n",
       "      <th>anti_valence</th>\n",
       "      <th>wbi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>zombie</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.704799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  arousal  anti_valence       wbi\n",
       "19999  zombie    0.648         0.786  0.704799"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check results ...\n",
    "display(lexicon.nlargest(10, 'wbi'))\n",
    "display(lexicon.loc[lexicon['word'] == 'zombie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, people nowadays definitely seem to be more scared of suicide bombers than of zombies ... how come?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Lyrics\n",
    "\n",
    "Scrape all Cannibal Corpse lyrics from the genius API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"https://api.genius.com\"\n",
    "genius_token = credentials.genius_token\n",
    "\n",
    "def get_json(path, params=None, headers=None):\n",
    "    '''Send request and get response in json format.'''\n",
    "\n",
    "    # Generate request URL\n",
    "    requrl = '/'.join([base, path])\n",
    "    token = f\"Bearer {genius_token}\"\n",
    "    if headers:\n",
    "        headers['Authorization'] = token\n",
    "    else:\n",
    "        headers = {\"Authorization\": token}\n",
    "    # Get response object from querying genius api\n",
    "    response = requests.get(url=requrl, params=params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get artist ID\n",
    "\n",
    "name = \"Cannibal Corpse\"\n",
    "\n",
    "def get_artist_id(artist_name):\n",
    "    '''Search Genius API for artist ID via artist name.'''\n",
    "\n",
    "    search = \"/search?q=\"\n",
    "    query = base + search + urllib.parse.quote(artist_name)\n",
    "    request = urllib.request.Request(query)\n",
    "    request.add_header(\"Authorization\", \"Bearer \" + genius_token)\n",
    "#     request.add_header(\"User-Agent\", \"\")  \n",
    "    response = urllib.request.urlopen(request, timeout=3)\n",
    "    raw = response.read()\n",
    "    data = json.loads(raw)['response']['hits']\n",
    "    \n",
    "    return (data[0]['result']['primary_artist']['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41863\n"
     ]
    }
   ],
   "source": [
    "artist_id = get_artist_id(name)\n",
    "print(artist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_songlist(artist_id):\n",
    "    '''Get all the song ids and titles from an artist in form of a dict.'''\n",
    "    current_page = 1\n",
    "    next_page = True\n",
    "    songs = [] # to store final song ids\n",
    "    while next_page:\n",
    "        path = f\"artists/{artist_id}/songs/\"\n",
    "        params = {'page': current_page} # the current page\n",
    "        data = get_json(path=path, params=params) # get json of songs\n",
    "        page_songs = data['response']['songs']\n",
    "        if page_songs:\n",
    "            # Add all the songs of current page\n",
    "            songs += page_songs\n",
    "            # Increment current_page value for next loop\n",
    "            current_page += 1\n",
    "            print(f\"Page {current_page} finished scraping\")\n",
    "            # If you don't wanna wait too long to scrape, un-comment this\n",
    "            # if current_page == 2:\n",
    "            #     break\n",
    "        else:\n",
    "            # If page_songs is empty, quit\n",
    "            next_page = False\n",
    "\n",
    "    print(f\"Song id were scraped from {current_page} pages\")\n",
    "\n",
    "    # Get all the song ids, excluding not-primary-artist songs.\n",
    "    songlist = {song[\"id\"]: song['title'] for song in songs\n",
    "                if song[\"primary_artist\"][\"id\"] == artist_id}\n",
    "\n",
    "    return songlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 2 finished scraping\n",
      "Page 3 finished scraping\n",
      "Page 4 finished scraping\n",
      "Page 5 finished scraping\n",
      "Page 6 finished scraping\n",
      "Page 7 finished scraping\n",
      "Page 8 finished scraping\n",
      "Page 9 finished scraping\n",
      "Page 10 finished scraping\n",
      "Page 11 finished scraping\n",
      "Song id were scraped from 11 pages\n",
      "[(764037, 'Absolute Hatred'), (715726, 'A Cauldron of Hate')]\n"
     ]
    }
   ],
   "source": [
    "songlist = get_songlist(artist_id)\n",
    "pprint(list(songlist.items())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_lyrics(song_id):\n",
    "    '''Constructs the path of song lyrics.'''\n",
    "\n",
    "    url = f\"songs/{song_id}\"\n",
    "    data = get_json(url)\n",
    "    # Gets the path of song lyrics\n",
    "    path = data['response']['song']['path']\n",
    "    return path\n",
    "\n",
    "def retrieve_lyrics(song_id):\n",
    "    '''Retrieves lyrics from html page.'''\n",
    "\n",
    "    path = connect_lyrics(song_id)\n",
    "    URL = \"http://genius.com\" + path\n",
    "    page = requests.get(URL)\n",
    "    # Extract the page's HTML as a string\n",
    "    html = BeautifulSoup(page.text, \"html.parser\")\n",
    "    # Scrape the song lyrics from the HTML\n",
    "    lyrics = html.find(\"div\", class_=\"lyrics\").get_text()\n",
    "    return lyrics\n",
    "\n",
    "def get_song_information(song_ids):\n",
    "    '''Retrieve meta data about a song.'''\n",
    "    # initialize a dictionary.\n",
    "    song_list = {}\n",
    "    print(\"Scraping song information\")\n",
    "    for i, song_id in enumerate(song_ids):\n",
    "        print(\"id:\" + str(song_id) + \" start. ->\")\n",
    "        path = \"songs/{}\".format(song_id)\n",
    "        data = get_json(path=path)[\"response\"][\"song\"]\n",
    "\n",
    "        song_list.update({\n",
    "        i: {\n",
    "            \"title\": data[\"title\"],\n",
    "            \"album\": data[\"album\"][\"name\"] if data[\"album\"] else \"<single>\",\n",
    "            \"release_date\": data[\"release_date\"] if data[\"release_date\"] else \"unidentified\",\n",
    "            \"featured_artists\":\n",
    "                [feat[\"name\"] if data[\"featured_artists\"] else \"\" for feat in data[\"featured_artists\"]],\n",
    "            \"producer_artists\":\n",
    "                [feat[\"name\"] if data[\"producer_artists\"] else \"\" for feat in data[\"producer_artists\"]],\n",
    "            \"writer_artists\":\n",
    "                [feat[\"name\"] if data[\"writer_artists\"] else \"\" for feat in data[\"writer_artists\"]],\n",
    "            \"genius_track_id\": song_id,\n",
    "            \"genius_album_id\": data[\"album\"][\"id\"] if data[\"album\"] else \"none\"}\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "        print(\"-> id:\" + str(song_id) + \" is finished. \\n\")\n",
    "\n",
    "    return song_list"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "sounds",
   "language": "python",
   "name": "sounds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
